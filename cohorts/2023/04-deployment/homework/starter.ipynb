{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c51efaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn==1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef880a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7836ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.bin', 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c08294",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    \n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4854399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669fda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = df[categorical].to_dict(orient='records')\n",
    "X_val = dv.transform(dicts)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a54c1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.28140357655334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the standard deviation of the prediction\n",
    "y_pred.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6ca136",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "month = 2\n",
    "\n",
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83b7a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.concat([pd.Series(df[\"ride_id\"]).reset_index(drop = True), pd.Series(y_pred).reset_index(drop = True)], axis=1).rename({0:\" pred\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efd1e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_file = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da5eaa9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to read from object of type: <class 'builtin_function_or_method'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pyarrow/_parquet.pyx:1718\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pyarrow/util.py:93\u001b[0m, in \u001b[0;36m_stringify_path\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnot a path-like object\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: not a path-like object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_result\u001b[39m.\u001b[39;49mto_parquet(\n\u001b[1;32m      3\u001b[0m     output_file,\n\u001b[1;32m      4\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpyarrow\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      6\u001b[0m     index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pandas/core/frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2889\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2890\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2891\u001b[0m     path,\n\u001b[1;32m   2892\u001b[0m     engine,\n\u001b[1;32m   2893\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2894\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2895\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2896\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2897\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2898\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    412\u001b[0m     df,\n\u001b[1;32m    413\u001b[0m     path_or_buf,\n\u001b[1;32m    414\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    415\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    416\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    417\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    418\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pandas/io/parquet.py:189\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    181\u001b[0m             table,\n\u001b[1;32m    182\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m         \u001b[39m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mwrite_table(\n\u001b[1;32m    190\u001b[0m             table, path_or_handle, compression\u001b[39m=\u001b[39;49mcompression, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pyarrow/parquet/core.py:3071\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m use_int96 \u001b[39m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   3070\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3071\u001b[0m     \u001b[39mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   3072\u001b[0m             where, table\u001b[39m.\u001b[39;49mschema,\n\u001b[1;32m   3073\u001b[0m             filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   3074\u001b[0m             version\u001b[39m=\u001b[39;49mversion,\n\u001b[1;32m   3075\u001b[0m             flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[1;32m   3076\u001b[0m             use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[1;32m   3077\u001b[0m             write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[1;32m   3078\u001b[0m             coerce_timestamps\u001b[39m=\u001b[39;49mcoerce_timestamps,\n\u001b[1;32m   3079\u001b[0m             data_page_size\u001b[39m=\u001b[39;49mdata_page_size,\n\u001b[1;32m   3080\u001b[0m             allow_truncated_timestamps\u001b[39m=\u001b[39;49mallow_truncated_timestamps,\n\u001b[1;32m   3081\u001b[0m             compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3082\u001b[0m             use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_int96,\n\u001b[1;32m   3083\u001b[0m             compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[1;32m   3084\u001b[0m             use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[1;32m   3085\u001b[0m             column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[1;32m   3086\u001b[0m             data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[1;32m   3087\u001b[0m             use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[1;32m   3088\u001b[0m             encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[1;32m   3089\u001b[0m             write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[1;32m   3090\u001b[0m             dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[1;32m   3091\u001b[0m             store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[1;32m   3092\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m   3093\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table, row_group_size\u001b[39m=\u001b[39mrow_group_size)\n\u001b[1;32m   3094\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pyarrow/parquet/core.py:990\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **options)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata_collector \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmetadata_collector\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    989\u001b[0m engine_version \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mV2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 990\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m _parquet\u001b[39m.\u001b[39;49mParquetWriter(\n\u001b[1;32m    991\u001b[0m     sink, schema,\n\u001b[1;32m    992\u001b[0m     version\u001b[39m=\u001b[39;49mversion,\n\u001b[1;32m    993\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    994\u001b[0m     use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[1;32m    995\u001b[0m     write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[1;32m    996\u001b[0m     use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_deprecated_int96_timestamps,\n\u001b[1;32m    997\u001b[0m     compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[1;32m    998\u001b[0m     use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[1;32m    999\u001b[0m     column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[1;32m   1000\u001b[0m     writer_engine_version\u001b[39m=\u001b[39;49mengine_version,\n\u001b[1;32m   1001\u001b[0m     data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[1;32m   1002\u001b[0m     use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[1;32m   1003\u001b[0m     encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[1;32m   1004\u001b[0m     write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[1;32m   1005\u001b[0m     dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[1;32m   1006\u001b[0m     store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[1;32m   1007\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m   1008\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_open \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pyarrow/_parquet.pyx:1720\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Data Science/GitHub/mlops-zoomcamp/venv/lib/python3.9/site-packages/pyarrow/io.pxi:1809\u001b[0m, in \u001b[0;36mpyarrow.lib.get_writer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to read from object of type: <class 'builtin_function_or_method'>"
     ]
    }
   ],
   "source": [
    "output_file = abs\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be32781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
