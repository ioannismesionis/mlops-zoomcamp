{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights & Biases workshop\n",
    "\n",
    "* Video: https://www.youtube.com/watch?v=yNyqFMwEyL4\n",
    "* Github repository: https://wandb.me/mlops-zoomcamp-github\n",
    "\n",
    "\n",
    "## Homework with Weights & Biases\n",
    "\n",
    "The goal of this homework is to get familiar with Weights & Biases for experiment tracking, model management, hyperparameter optimization, and many more.\n",
    "\n",
    "Before getting started with the homework, you need to have a Weights & Biases account. You can do so by visiting [wandb.ai/site](https://wandb.ai/site) and clicking on the **Sign Up** button.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Install the Package\n",
    "\n",
    "To get started with Weights & Biases you'll need to install the appropriate Python package.\n",
    "\n",
    "For this we recommend creating a separate Python environment, for example, you can use [conda environments](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html#managing-envs), \n",
    "and then install the package there with `pip` or `conda`.\n",
    "\n",
    "Following are the libraries you need to install:\n",
    "\n",
    "* `pandas`\n",
    "* `matplotlib`\n",
    "* `scikit-learn`\n",
    "* `pyarrow`\n",
    "* `wandb`\n",
    "\n",
    "Once you installed the package, run the command `wandb --version` and check the output.\n",
    "\n",
    "What's the version that you have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Download and preprocess the data\n",
    "\n",
    "We'll use the Green Taxi Trip Records dataset to predict the amount of tips for each trip. \n",
    "\n",
    "Download the data for January, February and March 2022 in parquet format from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "\n",
    "**Tip:** In case you're on [GitHub Codespaces](https://github.com/features/codespaces) or [gitpod.io](https://gitpod.io), you can open up the terminal and run the following commands to download the data:\n",
    "\n",
    "```shell\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet\n",
    "```\n",
    "\n",
    "Use the script `preprocess_data.py` located in the folder [`homework-wandb`](homework-wandb) to preprocess the data.\n",
    "\n",
    "The script will:\n",
    "\n",
    "* initialize a Weights & Biases run.\n",
    "* load the data from the folder `<TAXI_DATA_FOLDER>` (the folder where you have downloaded the data),\n",
    "* fit a `DictVectorizer` on the training set (January 2022 data),\n",
    "* save the preprocessed datasets and the `DictVectorizer` to your Weights & Biases dashboard as an artifact of type `preprocessed_dataset`.\n",
    "\n",
    "Your task is to download the datasets and then execute this command:\n",
    "\n",
    "```bash\n",
    "python preprocess_data.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --raw_data_path <TAXI_DATA_FOLDER> \\\n",
    "  --dest_path ./output\n",
    "```\n",
    "\n",
    "Tip: go to `02-experiment-tracking/homework-wandb/` folder before executing the command and change the value of `<WANDB_PROJECT_NAME>` to the name of your Weights & Biases project, `<WANDB_USERNAME>` to your Weights & Biases username, and `<TAXI_DATA_FOLDER>` to the location where you saved the data.\n",
    "\n",
    "Once you navigate to the `Files` tab of your artifact on your Weights & Biases page, what's the size of the saved `DictVectorizer` file?\n",
    "\n",
    "* 54 kB\n",
    "* 154 kB\n",
    "* 54 MB\n",
    "* 154 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Train a model with Weights & Biases logging\n",
    "\n",
    "We will train a `RandomForestRegressor` (from Scikit-Learn) on the taxi dataset.\n",
    "\n",
    "We have prepared the training script `train.py` for this exercise, which can be also found in the folder `homework-wandb`. \n",
    "\n",
    "The script will:\n",
    "\n",
    "* initialize a Weights & Biases run.\n",
    "* load the preprocessed datasets by fetching them from the Weights & Biases artifact previously created,\n",
    "* train the model on the training set,\n",
    "* calculate the MSE score on the validation set and log it to Weights & Biases,\n",
    "* save the trained model and log it to Weights & Biases as a model artifact.\n",
    "\n",
    "Your task is to modify the script to enable to add Weights & Biases logging, execute the script and then check the Weights & Biases run UI to check that the experiment run was properly tracked.\n",
    "\n",
    "TODO 1: log `mse` to Weights & Biases under the key `\"MSE\"`\n",
    "\n",
    "TODO 2: log `regressor.pkl` as an artifact of type `model`, refer to the [official docs](https://docs.wandb.ai/guides/artifacts) in order to know more about logging artifacts.\n",
    "\n",
    "You can run the script using:\n",
    "\n",
    "```bash\n",
    "python train.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --data_artifact \"<WANDB_USERNAME>/<WANDB_PROJECT_NAME>/NYC-Taxi:v0\"\n",
    "```\n",
    "\n",
    "Tip 1: You can find the artifact address under the `Usage` tab in the respective artifact's page.\n",
    "\n",
    "Tip 2: don't modify the hyperparameters of the model to make sure that the training will finish quickly.\n",
    "\n",
    "Once you have successfully ran the script, navigate the `Overview` section of the run in the Weights & Biases UI and scroll down to the `Configs`. What is the value of the `max_depth` parameter:\n",
    "\n",
    "* 4\n",
    "* 6\n",
    "* 8\n",
    "* 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Tune model hyperparameters\n",
    "\n",
    "Now let's try to reduce the validation error by tuning the hyperparameters of the `RandomForestRegressor` using [Weights & Biases Sweeps](https://docs.wandb.ai/guides/sweeps). We have prepared the script `sweep.py` for this exercise in the `homework-wandb` directory.\n",
    "\n",
    "Your task is to modify `sweep.py` to pass the parameters `n_estimators`, `min_samples_split` and `min_samples_leaf` from `config` to `RandomForestRegressor` inside the `run_train()` function. Then we will run the sweep to figure out not only the best best of hyperparameters for training our model, but also to analyze the most optimum trends in different hyperparameters. We can run the sweep using:\n",
    "\n",
    "```bash\n",
    "python sweep.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --data_artifact \"<WANDB_USERNAME>/<WANDB_PROJECT_NAME>/NYC-Taxi:v0\"\n",
    "```\n",
    "\n",
    "This command will run the sweep for 5 iterations using the **Bayesian Optimization and HyperBand** method proposed by the paper [BOHB: Robust and Efficient Hyperparameter Optimization at Scale](https://arxiv.org/abs/1807.01774). You can take a look at the sweep on your Weights & Biases dashboard, take a look at the **Parameter Inportance Panel** and the **Parallel Coordinates Plot** to determine, and analyze which hyperparameter is the most important:\n",
    "\n",
    "* `max_depth`\n",
    "* `n_estimators`\n",
    "* `min_samples_split`\n",
    "* `min_samples_leaf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Link the best model to the model registry\n",
    "\n",
    "Now that we have obtained the optimal set of hyperparameters and trained the best model, we can assume that we are ready to test some of these models in production. In this exercise, you'll create a model registry and link the best model from the Sweep to the model registry.\n",
    "\n",
    "First, you will need to create a Registered Model to hold all the candidate models for your particular modeling task. You can refer to [this section](https://docs.wandb.ai/guides/models/walkthrough#1-create-a-new-registered-model) of the official docs to learn how to create a registered model using the Weights & Biases UI.\n",
    "\n",
    "Once you have created the Registered Model successfully, you can navigate to the best run of your sweep, navigate to the model artifact created by the particular run, and click on the Link to Registry option from the UI. This would link the model artifact to the Registered Model. You can choose to add some suitable aliases for the Registered Model, such as `production`, `best`, etc.\n",
    "\n",
    "Now that the model artifact is linked to the Registered Model, which of these information do we see on the Registered Model UI?\n",
    "\n",
    "* Versioning\n",
    "* Metadata\n",
    "* Aliases\n",
    "* Metric (MSE)\n",
    "* Source run\n",
    "* All of these\n",
    "* None of these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://forms.gle/ndmTHeogFLeckSHm9\n",
    "* You can submit your solution multiple times. In this case, only the last submission will be used\n",
    "* If your answer doesn't match options exactly, select the closest one\n",
    "\n",
    "\n",
    "## Deadline\n",
    "\n",
    "The deadline for submitting is 6 June, 23:00 (Berlin time). \n",
    "\n",
    "After that, the form will be closed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
